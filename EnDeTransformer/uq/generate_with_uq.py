from dataclasses import dataclass
from sacrebleu import BLEU
from sympy import hyper
import torch
import torch.nn as nn
from tqdm import tqdm
from EnDeTransformer.beam_search import AutoregressiveInferenceResults, beam_search_batched, greedy_search, top_k_sampling
from EnDeTransformer.data_processing.vocab import BOS_TOKEN, Vocabulary, load_vocab, output_to_text
from EnDeTransformer.uq.acquisition_func import AcquisitionFunction, BLEU_mean_output_batch
from EnDeTransformer.hyperparameters import hyperparameters
from EnDeTransformer.constants import constants
from typing import List, Tuple, Union

@dataclass
class BatchedValidationResult:
    """
    Sentences generated by the model (batch_size, max_len)
    """
    hypothesis: List[str]
    """
    References for the generated sentences (batch_size, max_len)
    """
    reference: List[str]
    """
    Uncertainty values for the generated sentences (batch_size,)
    """
    uncertainty: torch.Tensor

def generate_autoregressivly_with_uq(
    model: nn.Module,
    src_tokens: torch.Tensor,
    ground_truth: torch.Tensor,
    print_ex: int,
    sample_beam_greed: str,
    aq_funcs: List[AcquisitionFunction],
    enable_dropout: bool,
) -> List[BatchedValidationResult]:
    vocab = load_vocab(constants.file_paths.vocab)
    model.eval()
    if enable_dropout:
        _enable_test_time_dropout(model)
    if sample_beam_greed == "beam":
        beam_search_function = beam_search_batched
    elif sample_beam_greed == "sample":
        beam_search_function = top_k_sampling
    else:
        beam_search_function = greedy_search

    batch_size = src_tokens.size(0)
    hypothesis :List[List[str]] = [[] for _ in range(batch_size)]
    token_ids : torch.Tensor = torch.zeros(batch_size, hyperparameters.uq.num_inferences, hyperparameters.transformer.max_len).to(hyperparameters.device)
    softmax_probs : torch.Tensor = torch.zeros(batch_size, hyperparameters.uq.num_inferences, hyperparameters.transformer.max_len).to(hyperparameters.device)

    for n in tqdm(range(hyperparameters.uq.num_inferences)):
        inference_result : AutoregressiveInferenceResults = beam_search_function(
            model,
            src_tokens,
            vocab,
        )
        token_ids[:, n, :] = inference_result.token_ids
        softmax_probs[:, n, :] = inference_result.get_softmax_probs_for_selected_token()
        for b in range(batch_size):
            hypothesis[b].append(output_to_text(inference_result.token_ids[b].tolist(), lang='en'))

    validation_result = []
    
    for aq_func in aq_funcs:
        if aq_func.multiple_inference:
            hyp = BLEU_mean_output_batch(hypothesis)
        else:
            hyp = [hypothesis[b][0] for b in range(batch_size)]
        ref = [output_to_text(ref) for ref in ground_truth.tolist()]
        uq = aq_func(hypothesis, token_ids, softmax_probs)

        validation_result.append(BatchedValidationResult(hyp, ref, uq))
    
    model.eval() # Disable dropout
    return validation_result

    
def _enable_test_time_dropout(model: nn.Module) -> None:
    for module in model.modules():
        if isinstance(module, nn.Dropout):
            module.train()

def print_sample_sentences(batch_size: int, src_tokens: torch.Tensor, ground_truth: torch.Tensor, text_output: Union[List[str], List[List[str]]], uq: torch.Tensor, aq_func: AcquisitionFunction, print_ex: int) -> None:

    random_indices = torch.randperm(batch_size)[:print_ex]
    for i in random_indices:
        
        print(f"Example {i+1} in batch")
        print(f"Source: {output_to_text(src_tokens[i].tolist(), lang='de')}")
        print(f"Ground truth: {output_to_text(ground_truth[i].tolist())}")
        # print(f"Source tokens: {src_tokens[i, 0].tolist()}")
        if aq_func.multiple_inference:
            for n in range(aq_func.num_inferences):
                print(f"Inference {n+1}:")
                print(f"Generated text: {text_output[i][n]}")
                # print(f"Generated tokens: {tgt_tokens[i, n].tolist()}")
                print("")
            print("=====")
        else:
            # print(f"Ground truth tokens: {tgt_tokens[i].tolist()}")
            print(f"Generated text: {text_output[i]}")
        print(f"Uncertainty: {uq[i]}")
        # print(f"Generated tokens: {tgt_tokens[i].tolist()}")
        print("")