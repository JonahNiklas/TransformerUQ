{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy test loss = 10.64527416229248\n",
      "Dummy test loss on own model = 10.447006225585938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sondr\\TransformerUQ\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.transformer_pytorch import TransformerPyTorch\n",
    "from models.transformer import Transformer\n",
    "from hyperparameters import hyperparameters\n",
    "\n",
    "vocab_size = 32000\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "pytorch_model: nn.Module = TransformerPyTorch(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=hyperparameters.transformer.hidden_size,\n",
    "    num_heads=hyperparameters.transformer.num_heads,\n",
    "    d_ff=hyperparameters.transformer.encoder_ffn_embed_dim,\n",
    "    num_encoder_layers=hyperparameters.transformer.num_hidden_layers,\n",
    "    num_decoder_layers=hyperparameters.transformer.num_hidden_layers,\n",
    "    dropout=hyperparameters.transformer.dropout,\n",
    "    max_len=hyperparameters.transformer.max_len,\n",
    ")\n",
    "own_model = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    d_model=hyperparameters.transformer.hidden_size,\n",
    "    num_heads=hyperparameters.transformer.num_heads,\n",
    "    d_ff=hyperparameters.transformer.encoder_ffn_embed_dim,\n",
    "    num_encoder_layers=hyperparameters.transformer.num_hidden_layers,\n",
    "    num_decoder_layers=hyperparameters.transformer.num_hidden_layers,\n",
    "    dropout=hyperparameters.transformer.dropout,\n",
    "    max_len=hyperparameters.transformer.max_len,\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction=\"mean\")\n",
    "\n",
    "# Dummy data\n",
    "src = torch.randint(1, vocab_size, (batch_size, seq_len))\n",
    "tgt = torch.randint(1, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Ensure no zeros in the middle (just for clarity)\n",
    "# but you can keep them if you want to test pad\n",
    "decoder_in = tgt[:, :-1]\n",
    "labels = tgt[:, 1:]\n",
    "\n",
    "logits = pytorch_model(src, decoder_in)  # shape [B, T-1, vocab_size]\n",
    "logits = logits.transpose(1, 2)  # shape [B, vocab_size, T-1]\n",
    "\n",
    "loss = criterion(logits, labels)  # shape [B, T-1]\n",
    "print(\"Dummy test loss =\", loss.item())\n",
    "\n",
    "# Own model\n",
    "logits = own_model(src, decoder_in)  # shape [B, T-1, vocab_size]\n",
    "logits = logits.transpose(1, 2)  # shape [B, vocab_size, T-1]\n",
    "\n",
    "loss = criterion(logits, labels)  # shape [B, T-1]\n",
    "print(\"Dummy test loss on own model =\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length in the dataset = 30.32287386028867\n"
     ]
    }
   ],
   "source": [
    "# Find average sentence length in the dataset\n",
    "merged_path = \"local/data/training/bpe_train.de\"\n",
    "total_len = 0\n",
    "num_lines = 0\n",
    "\n",
    "with open(merged_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        total_len += len(line.split())\n",
    "        num_lines += 1\n",
    "\n",
    "avg_len = total_len / num_lines\n",
    "print(\"Average sentence length in the dataset =\", avg_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 32181\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "from vocab import Vocabulary\n",
    "\n",
    "\n",
    "vocab = pickle.load(open(\"local/vocab_shared.pkl\", \"rb\")) # type: ignore\n",
    "print(\"Vocab size =\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5186,  0.5575],\n",
      "         [-0.5299,  0.1968]],\n",
      "\n",
      "        [[-0.7712, -1.9582],\n",
      "         [-1.8723,  0.8392]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "    [\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "    ]\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DropoutEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, dropout: float, padding_idx: int) -> None:\n",
    "        \"\"\"\n",
    "        Applies dropout to entire rows of the embedding matrix.\n",
    "        \n",
    "        Args:\n",
    "            num_embeddings (int): number of embeddings (vocabulary size).\n",
    "            embedding_dim (int): dimension of each embedding vector.\n",
    "            dropout (float): probability of dropping an entire embedding row.\n",
    "            padding_idx (int): index of the padding token (never dropped).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        # When training, apply dropout to the embedding weights.\n",
    "        if self.training and self.dropout > 0:\n",
    "            weight = self.embedding.weight  # shape: [num_embeddings, embedding_dim]\n",
    "            # Create a dropout mask for rows: shape: [num_embeddings, 1]\n",
    "            mask = weight.new_empty((weight.size(0), 1)).bernoulli_(1 - self.dropout)\n",
    "            # Scale the surviving rows to maintain expected values\n",
    "            mask = mask / (1 - self.dropout)\n",
    "            # Make sure that the padding index is always kept.\n",
    "            if self.embedding.padding_idx is not None:\n",
    "                mask[self.embedding.padding_idx] = 1\n",
    "            # Apply the mask to zero out (drop) entire rows.\n",
    "            dropped_weight = weight * mask\n",
    "            # Use the masked weights for the embedding lookup.\n",
    "            return F.embedding(\n",
    "                input,\n",
    "                dropped_weight,\n",
    "                self.embedding.padding_idx,\n",
    "                self.embedding.max_norm,\n",
    "                self.embedding.norm_type,\n",
    "                self.embedding.scale_grad_by_freq,\n",
    "                self.embedding.sparse,\n",
    "            )\n",
    "        else:\n",
    "            # In evaluation mode (or if dropout == 0), use the regular embedding.\n",
    "            return self.embedding(input)\n",
    "\n",
    "\n",
    "# dropout\n",
    "dropout_embedding = DropoutEmbedding(10, 2, 0.1, 0)\n",
    "print(dropout_embedding(x, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.48019047027906"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weakref import ref\n",
    "import sacrebleu\n",
    "\n",
    "ref_sentence = \"Hello, I am a boy\"\n",
    "pred_sentence = \"Hi, I am a big boy\"\n",
    "\n",
    "# bleu = sacrebleu.sentence_bleu(pred_sentence, [ref_sentence])\n",
    "bleu = sacrebleu.sentence_bleu(ref_sentence, [pred_sentence])\n",
    "bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-inf)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.log(torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1577, 0.1577, 0.2076, 0.1577, 0.1617, 0.1577])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "logits= torch.tensor([0.05, 0.05, 0.6, 0.05, 0.1, 0.05])\n",
    "temperature = 2.0\n",
    "torch.softmax(logits / temperature, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ConcreteDropout.forward() missing 1 required positional argument: 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m], [\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m]])\n\u001b[0;32m     68\u001b[0m cd \u001b[38;5;241m=\u001b[39m ConcreteDropout()\n\u001b[1;32m---> 69\u001b[0m \u001b[43mcd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sondr\\TransformerUQ\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sondr\\TransformerUQ\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: ConcreteDropout.forward() missing 1 required positional argument: 'layer'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class ConcreteDropout(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weight_regularizer=1e-6,\n",
    "        dropout_regularizer=1e-5,\n",
    "        init_min=0.1,\n",
    "        init_max=0.1,\n",
    "    ):\n",
    "        super(ConcreteDropout, self).__init__()\n",
    "\n",
    "        self.weight_regularizer = weight_regularizer\n",
    "        self.dropout_regularizer = dropout_regularizer\n",
    "\n",
    "        init_min = np.log(init_min) - np.log(1.0 - init_min)\n",
    "        init_max = np.log(init_max) - np.log(1.0 - init_max)\n",
    "\n",
    "        self.p_logit = nn.Parameter(torch.empty(1).uniform_(init_min, init_max))\n",
    "\n",
    "    def forward(self, x, layer):\n",
    "        p = torch.sigmoid(self.p_logit)\n",
    "\n",
    "        out = layer(self._concrete_dropout(x, p))\n",
    "\n",
    "        sum_of_square = 0\n",
    "        for param in layer.parameters():\n",
    "            sum_of_square += torch.sum(torch.pow(param, 2))\n",
    "\n",
    "        weights_regularizer = self.weight_regularizer * sum_of_square / (1 - p)\n",
    "\n",
    "        dropout_regularizer = p * torch.log(p)\n",
    "        dropout_regularizer += (1.0 - p) * torch.log(1.0 - p)\n",
    "\n",
    "        input_dimensionality = x[0].numel()  # Number of elements of first item in batch\n",
    "        dropout_regularizer *= self.dropout_regularizer * input_dimensionality\n",
    "\n",
    "        regularization = weights_regularizer + dropout_regularizer\n",
    "        return out, regularization\n",
    "\n",
    "    def _concrete_dropout(self, x, p):\n",
    "        eps = 1e-7\n",
    "        temp = 0.1\n",
    "\n",
    "        unif_noise = torch.rand_like(x)\n",
    "\n",
    "        drop_prob = (\n",
    "            torch.log(p + eps)\n",
    "            - torch.log(1 - p + eps)\n",
    "            + torch.log(unif_noise + eps)\n",
    "            - torch.log(1 - unif_noise + eps)\n",
    "        )\n",
    "\n",
    "        drop_prob = torch.sigmoid(drop_prob / temp)\n",
    "        random_tensor = 1 - drop_prob\n",
    "        retain_prob = 1 - p\n",
    "\n",
    "        x = torch.mul(x, random_tensor)\n",
    "        x /= retain_prob\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "cd = ConcreteDropout()\n",
    "cd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConcreteDropoutHyperparameters(l=0.01, weight_regularizer=2.172223431426599e-11, dropout_regularizer=4.344446862853198e-07)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ConcreteDropoutHyperparameters(BaseModel):\n",
    "    l: float = 0.01 # Try also 0.1 and 0.001\n",
    "    _number_of_training_examples: int = 4_603_578\n",
    "    weight_regularizer: float = l**2 / _number_of_training_examples\n",
    "    dropout_regularizer: float = 2 / (_number_of_training_examples)\n",
    "\n",
    "ConcreteDropoutHyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
