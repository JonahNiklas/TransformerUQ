from __future__ import annotations

from dataclasses import dataclass
import logging
from typing import List, Tuple
import torch
import torch.nn as nn
import torch.utils.data as data
from sacrebleu import corpus_bleu
from tqdm import tqdm

from data_processing.vocab import output_to_text
from hyperparameters import hyperparameters
from uq.generate_with_uq import BatchedValidationResult, generate_autoregressivly_with_uq
from uq.acquisition_func import AcquisitionFunction, BLEUVariance

logger = logging.getLogger(__name__)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

@dataclass
class ValidationResult:
    """
    Sentences generated by the model (test_size, max_len)
    """
    hypothesis: List[str]
    """
    References for the generated sentences (test_size, max_len)
    """
    reference: List[str]
    """
    Uncertainty values for the generated sentences (test_size,)
    """
    uncertainty: torch.Tensor
    

def validate_uq(
    model: nn.Module,
    test_data: data.DataLoader,
    sample_beam_greed: str,
    aq_funcs: List[AcquisitionFunction],
    enable_dropout: bool,
    save_hypotheses_to_file: bool = False,
    num_batches_to_validate_on: int | None = None,
    print_ex: int = 1,
) -> List[ValidationResult]:
    
    output_list: List[ValidationResult] = [ValidationResult(hypothesis=[], reference=[], uncertainty=torch.tensor([]).to(hyperparameters.device)) for _ in aq_funcs]

    logger.debug("Started validating models")

    with torch.no_grad():
        for i, batch in tqdm(
            enumerate(test_data), desc="Running validation", total=len(test_data)
        ):
            src_tokens, ground_truth = batch
            src_tokens, ground_truth = src_tokens.to(device), ground_truth.to(device)
            
            batched_validation_results : List[BatchedValidationResult] = generate_autoregressivly_with_uq(model, src_tokens, ground_truth, print_ex, sample_beam_greed, aq_funcs,enable_dropout)
            
            for j, aq_func in enumerate(aq_funcs):
                output_list[j].hypothesis.extend(batched_validation_results[j].hypothesis)
                output_list[j].reference.extend(batched_validation_results[j].reference)
                output_list[j].uncertainty = torch.cat([output_list[j].uncertainty, batched_validation_results[j].uncertainty], dim=0)
            

            if num_batches_to_validate_on is not None and i + 1 >= num_batches_to_validate_on:
                logger.info(f"Only validating on {num_batches_to_validate_on} batches, stopping")
                break

    return output_list
   


if __name__ == "__main__":
    dummy_hyptheses = [
        "This is a string for BLEU metric computation",
        "Banana is nice for health",
        "cat makes sounds"
    ]
    dummy_references = [
        "This is a test sentence for BLEU score calculation",
        "Banana is good for a long life",
        "cat is meowing",
        "house is big",
    ]

    bleu_score = corpus_bleu(dummy_hyptheses, [dummy_references]).score
    print(f"Dummy BLEU Score: {bleu_score}")